{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:08:45.465910Z",
     "iopub.status.busy": "2025-05-02T08:08:45.465520Z",
     "iopub.status.idle": "2025-05-02T08:08:45.472303Z",
     "shell.execute_reply": "2025-05-02T08:08:45.470727Z",
     "shell.execute_reply.started": "2025-05-02T08:08:45.465871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAIN_SEED = 493\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# SEQ_LENGTH = 60\n",
    "\n",
    "MODEL_DIM = 56\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "FFN_DIM = 88 # [256, 512]\n",
    "LR = 7.456901799768986e-06\n",
    "EPOCH = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in c:\\anaconda\\lib\\site-packages (4.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-02T08:08:45.474251Z",
     "iopub.status.busy": "2025-05-02T08:08:45.473792Z",
     "iopub.status.idle": "2025-05-02T08:08:52.260104Z",
     "shell.execute_reply": "2025-05-02T08:08:52.259011Z",
     "shell.execute_reply.started": "2025-05-02T08:08:45.474208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    # Set seed for reproducibility\n",
    "    random.seed(seed_value)  # Python's random module\n",
    "    np.random.seed(seed_value)  # NumPy\n",
    "    torch.manual_seed(seed_value)  # PyTorch\n",
    "    torch.cuda.manual_seed(seed_value)  # GPU (if available)\n",
    "    torch.cuda.manual_seed_all(seed_value)  # For multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic algorithms\n",
    "    torch.backends.cudnn.benchmark = False  # Disable benchmark mode for reproducibility\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(MAIN_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:08:52.262957Z",
     "iopub.status.busy": "2025-05-02T08:08:52.262294Z",
     "iopub.status.idle": "2025-05-02T08:08:52.273688Z",
     "shell.execute_reply": "2025-05-02T08:08:52.272439Z",
     "shell.execute_reply.started": "2025-05-02T08:08:52.262925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, labels, seq_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: NumPy array of shape (N, T, 19), where N is the number of sequences\n",
    "            labels: NumPy array of shape (N, 19) for classification\n",
    "            seq_length: Length of sequences (T)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "def preprocess_data(raw_data, raw_labels, seq_length, batch_size, shuffle):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        raw_data: NumPy array of shape (total_timesteps, 19)\n",
    "        raw_labels: NumPy array of shape (total_timesteps, 19)\n",
    "        seq_length: The length of sequences (T)\n",
    "    \n",
    "    Returns:\n",
    "        PyTorch DataLoader for training\n",
    "    \"\"\"\n",
    "\n",
    "    # Create overlapping sequences\n",
    "    data_sequences = []\n",
    "    label_sequences = []\n",
    "    for i in range(len(raw_data) - seq_length):\n",
    "        data_sequences.append(raw_data[i:i+seq_length].astype(np.float32))\n",
    "        label_sequences.append(raw_labels[i + seq_length].astype(np.float32))  # Predict based on last time step\n",
    "\n",
    "    data_sequences = np.array(data_sequences)  # Shape (N, T, 19)\n",
    "    label_sequences = np.array(label_sequences)  # Shape (N, 19)\n",
    "\n",
    "    dataset = TimeSeriesDataset(data_sequences, label_sequences, seq_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def create_lstm_dataset(df, sequence_length=12, step=20):\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range((sequence_length - 1) * step, len(df)):  # Ensure enough past data for X\n",
    "        x_i = []\n",
    "        for j in range(sequence_length):\n",
    "            index = i - (sequence_length - 1 - j) * step  # Compute the index for each row in X_i\n",
    "            if index < 0:\n",
    "                break  # Stop if we run out of data\n",
    "            x_i.append(df[index, :19])  # Select return rates of stock1, stock2, stock3\n",
    "\n",
    "        if len(x_i) == sequence_length:  # Only append fully constructed sequences\n",
    "            X.append(np.array(x_i))\n",
    "            y.append(df[i, 19:])  # Take y from the same index as the last row of X_i\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:08:52.276072Z",
     "iopub.status.busy": "2025-05-02T08:08:52.275662Z",
     "iopub.status.idle": "2025-05-02T08:08:55.729545Z",
     "shell.execute_reply": "2025-05-02T08:08:55.728313Z",
     "shell.execute_reply.started": "2025-05-02T08:08:52.276032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data shape after split\n",
      "X_train: torch.Size([2533, 12, 19])\n",
      "y_train: torch.Size([2533, 19])\n",
      "X_val: torch.Size([698, 12, 19])\n",
      "y_val: torch.Size([698, 19])\n",
      "X_test: torch.Size([698, 12, 19])\n",
      "y_test: torch.Size([698, 19])\n",
      "\n",
      " Dataset:\n",
      "train: 80\n",
      "val: 22\n",
      "test: 22\n",
      "\n",
      "Data shape after split\n",
      "X_train: torch.Size([2533, 12, 19])\n",
      "y_train: torch.Size([2533, 19])\n",
      "X_val: torch.Size([698, 12, 19])\n",
      "y_val: torch.Size([698, 19])\n",
      "X_test: torch.Size([698, 12, 19])\n",
      "y_test: torch.Size([698, 19])\n",
      "\n",
      " Dataset:\n",
      "train: 80\n",
      "val: 22\n",
      "test: 22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sheet_url = \"https://docs.google.com/spreadsheets/d/1vXUzWW-6toJxGv8DL5I9RTlq-vcrjCGQcUuFQQtjJbY/export?format=csv&gid=0\" # train_data_daily_monthly_3_2\n",
    "df = pd.read_csv(sheet_url)\n",
    "    \n",
    "df_ret_original = df.iloc[:, 1:20]\n",
    "df_bm_original  = df.iloc[:, 61:80]\n",
    "\n",
    "df_ret = df.iloc[:, 1:20]\n",
    "df_out = df.iloc[:, 21:40]\n",
    "df_m = df.iloc[:, 41:60]\n",
    "df_bm = df.iloc[:, 61:80]\n",
    "df_b = df.iloc[:, 81:100]\n",
    "\n",
    "df_selected_list = [df_bm.copy()] # which (combination of) indicator to use\n",
    "df_selected = pd.concat(df_selected_list, axis=1) \n",
    "INPUT_DIM = df_selected.shape[1]\n",
    "\n",
    "train_split_idx = math.floor(df_selected.shape[0] * 0.6)\n",
    "val_split_idx = math.floor(df_selected.shape[0] * 0.8)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, X_test = (\n",
    "    df_selected.iloc[:train_split_idx, :],  # Train set (0% -> 60%)\n",
    "    df_selected.iloc[train_split_idx:val_split_idx, :],  # Validation set (60% -> 80%)\n",
    "    df_selected.iloc[val_split_idx:, :]  # Test set (80% -> 100%)\n",
    ")\n",
    "\n",
    "df_y_train, df_y_val, df_y_test = (\n",
    "    df_out.iloc[:train_split_idx, :],\n",
    "    df_out.iloc[train_split_idx:val_split_idx, :],\n",
    "    df_out.iloc[val_split_idx:, :]\n",
    ")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "df_selected_aug_train = np.hstack([X_train_scaled, df_y_train])\n",
    "X_train, y_train = create_lstm_dataset(df_selected_aug_train)\n",
    "\n",
    "df_selected_aug_val = np.hstack([X_val_scaled, df_y_val])\n",
    "X_val, y_val = create_lstm_dataset(df_selected_aug_val)\n",
    "\n",
    "df_selected_aug_test = np.hstack([X_test_scaled, df_y_test])\n",
    "X_test, y_test = create_lstm_dataset(df_selected_aug_test)\n",
    "\n",
    "\n",
    "print(\"\\nData shape after split\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"y_val:\", y_val.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"\\n Dataset:\")\n",
    "print(\"train:\", len(train_dataloader))\n",
    "print(\"val:\", len(val_dataloader))\n",
    "print(\"test:\", len(test_dataloader))\n",
    "\n",
    "print(\"\\nData shape after split\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"y_val:\", y_val.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "\n",
    "print(\"\\n Dataset:\")\n",
    "print(\"train:\", len(train_dataloader))\n",
    "print(\"val:\", len(val_dataloader))\n",
    "print(\"test:\", len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:08:55.731318Z",
     "iopub.status.busy": "2025-05-02T08:08:55.730943Z",
     "iopub.status.idle": "2025-05-02T08:08:55.741476Z",
     "shell.execute_reply": "2025-05-02T08:08:55.740159Z",
     "shell.execute_reply.started": "2025-05-02T08:08:55.731279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, num_classes, ffn_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.positional_encoding = PositionalEncoding(model_dim, dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ffn_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(model_dim, num_classes)  # Output num_classes for classification\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_length, model_dim)\n",
    "        x = self.positional_encoding(x)  # Add positional encoding\n",
    "        x = self.transformer_encoder(x)  # Transformer encoding\n",
    "        x = self.fc(x[:, -1, :])  # Take last timestep's output for classification\n",
    "        out = self.softmax(x)  # Apply softmax to the output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:08:55.742904Z",
     "iopub.status.busy": "2025-05-02T08:08:55.742594Z",
     "iopub.status.idle": "2025-05-02T08:09:48.118257Z",
     "shell.execute_reply": "2025-05-02T08:09:48.117160Z",
     "shell.execute_reply.started": "2025-05-02T08:08:55.742878Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 71179\n",
      "Epoch 1/27, Train Loss: 0.3068, Val Loss: 0.3065\n",
      "Epoch 2/27, Train Loss: 0.3063, Val Loss: 0.3064\n",
      "Epoch 3/27, Train Loss: 0.3061, Val Loss: 0.3063\n",
      "Epoch 4/27, Train Loss: 0.3058, Val Loss: 0.3062\n",
      "Epoch 5/27, Train Loss: 0.3056, Val Loss: 0.3061\n",
      "Epoch 6/27, Train Loss: 0.3054, Val Loss: 0.3061\n",
      "Epoch 7/27, Train Loss: 0.3053, Val Loss: 0.3060\n",
      "Epoch 8/27, Train Loss: 0.3052, Val Loss: 0.3060\n",
      "Epoch 9/27, Train Loss: 0.3051, Val Loss: 0.3060\n",
      "Epoch 10/27, Train Loss: 0.3050, Val Loss: 0.3059\n",
      "Epoch 11/27, Train Loss: 0.3048, Val Loss: 0.3059\n",
      "Epoch 12/27, Train Loss: 0.3047, Val Loss: 0.3059\n",
      "Epoch 13/27, Train Loss: 0.3047, Val Loss: 0.3059\n",
      "Epoch 14/27, Train Loss: 0.3046, Val Loss: 0.3059\n",
      "Epoch 15/27, Train Loss: 0.3046, Val Loss: 0.3059\n",
      "Epoch 16/27, Train Loss: 0.3045, Val Loss: 0.3058\n",
      "Epoch 17/27, Train Loss: 0.3045, Val Loss: 0.3058\n",
      "Epoch 18/27, Train Loss: 0.3044, Val Loss: 0.3058\n",
      "Epoch 19/27, Train Loss: 0.3043, Val Loss: 0.3058\n",
      "Epoch 20/27, Train Loss: 0.3043, Val Loss: 0.3058\n",
      "Epoch 21/27, Train Loss: 0.3042, Val Loss: 0.3058\n",
      "Epoch 22/27, Train Loss: 0.3042, Val Loss: 0.3058\n",
      "Epoch 23/27, Train Loss: 0.3041, Val Loss: 0.3057\n",
      "Epoch 24/27, Train Loss: 0.3041, Val Loss: 0.3057\n",
      "Epoch 25/27, Train Loss: 0.3040, Val Loss: 0.3057\n",
      "Epoch 26/27, Train Loss: 0.3039, Val Loss: 0.3057\n",
      "Epoch 27/27, Train Loss: 0.3040, Val Loss: 0.3057\n"
     ]
    }
   ],
   "source": [
    "model = TransformerClassifier(input_dim=INPUT_DIM,    # input값의 dim (1 or 2 or 3)\n",
    "                              model_dim=MODEL_DIM,    # 모델에서 사용되는 dim\n",
    "                              num_heads=NUM_HEADS,    # 헤드 개수\n",
    "                              num_layers=NUM_LAYERS,  # \n",
    "                              num_classes=19,         # output 벡터의 차원\n",
    "                              ffn_dim=FFN_DIM,        # \n",
    "                              dropout=0.1             # dropout rate\n",
    "                              )\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {count_parameters(model)}\")\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # (batch_size, num_classes)\n",
    "\n",
    "        # Compute loss (MSE between logits and one-hot target)\n",
    "        loss = criterion(outputs, targets)  # targets are one-hot encoded\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCH}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"transformer_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:09:48.119823Z",
     "iopub.status.busy": "2025-05-02T08:09:48.119243Z",
     "iopub.status.idle": "2025-05-02T08:09:48.232193Z",
     "shell.execute_reply": "2025-05-02T08:09:48.230767Z",
     "shell.execute_reply.started": "2025-05-02T08:09:48.119792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transform_array(arr):\n",
    "    result = np.zeros_like(arr)  # Initialize an array filled with 0s\n",
    "\n",
    "    # Get indices of 4 largest and 4 smallest values in each row\n",
    "    sorted_indices = np.argsort(arr, axis=1)  # Sort indices along each row\n",
    "\n",
    "    lowest_indices = sorted_indices[:, :4]   # First 4 indices (smallest values)\n",
    "    highest_indices = sorted_indices[:, -4:] # Last 4 indices (largest values)\n",
    "\n",
    "    # Assign -1 to lowest values\n",
    "    np.put_along_axis(result, lowest_indices, -1, axis=1)\n",
    "    # Assign 1 to highest values\n",
    "    np.put_along_axis(result, highest_indices, 1, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "model = TransformerClassifier(input_dim=3, model_dim=16, num_heads=4, num_layers=4, ffn_dim=16)  # Initialize model\n",
    "model.load_state_dict(torch.load(\"transformer_model.pth\"))  # Load saved weights\n",
    "model.eval()  # Set to evaluation mode\n",
    "\"\"\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "output_list = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_dataloader:\n",
    "        outputs = model(inputs)  # Get logits\n",
    "        output_list.append(outputs.numpy())\n",
    "\n",
    "y_pred = np.vstack(output_list)\n",
    "y_pos_pred = transform_array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:09:48.234874Z",
     "iopub.status.busy": "2025-05-02T08:09:48.234417Z",
     "iopub.status.idle": "2025-05-02T08:09:48.409715Z",
     "shell.execute_reply": "2025-05-02T08:09:48.408493Z",
     "shell.execute_reply.started": "2025-05-02T08:09:48.234842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Panel A : Mar. 2022 - Feb. 2025\n",
      "H4 strategy\n",
      "(Monthly)  Avg. Ret. = 0.02721417727507163\n",
      "(Annual) Avg. Ret. = 0.32657012730085955\n",
      "\n",
      "L4 strategy\n",
      "(Monthly)  Avg. Ret. = -0.014617385403295128\n",
      "(Annual) Avg. Ret. = -0.17540862483954153\n",
      "\n",
      "H4-L4 strategy\n",
      "(Monthly)  Avg. Ret. = 0.04183156267836676\n",
      "(Annual) Avg. Ret. = 0.5019787521404011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nPanel A : Mar. 2022 - Feb. 2025\")\n",
    "\n",
    "# Calculate H4 return\n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx:, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy()\n",
    "signals.replace(-1, 0, inplace=True)\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"H4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_h4.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate L4 return\n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx:, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy()\n",
    "signals.replace(1, 0, inplace=True)\n",
    "signals.replace(-1, 1, inplace=True)\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"L4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_l4.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate H4-L4 return\n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx:, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy()\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "# print(signals.sum())\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"H4-L4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_h4_l4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:09:48.411279Z",
     "iopub.status.busy": "2025-05-02T08:09:48.410984Z",
     "iopub.status.idle": "2025-05-02T08:09:48.650853Z",
     "shell.execute_reply": "2025-05-02T08:09:48.649856Z",
     "shell.execute_reply.started": "2025-05-02T08:09:48.411255Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Panel B : Mar. 2022 - Feb. 2023\n",
      "H4 strategy\n",
      "(Monthly)  Avg. Ret. = 0.011103964396551725\n",
      "(Annual) Avg. Ret. = 0.1332475727586207\n",
      "\n",
      "L4 strategy\n",
      "(Monthly)  Avg. Ret. = -0.023539174429956896\n",
      "(Annual) Avg. Ret. = -0.28247009315948274\n",
      "\n",
      "H4-L4 strategy\n",
      "(Monthly)  Avg. Ret. = 0.03464313882650862\n",
      "(Annual) Avg. Ret. = 0.41571766591810344\n",
      "\n",
      "(Annual)  Sharpe = 1.8448880017800633\n",
      "\n",
      "\n",
      "Panel C : Mar. 2023 - Jan. 2024\n",
      "H4 strategy\n",
      "(Monthly)  Avg. Ret. = 0.024177178348060345\n",
      "(Annual) Avg. Ret. = 0.2901261401767241\n",
      "\n",
      "L4 strategy\n",
      "(Monthly)  Avg. Ret. = -0.01738136977047414\n",
      "(Annual) Avg. Ret. = -0.2085764372456897\n",
      "\n",
      "H4-L4 strategy\n",
      "(Monthly)  Avg. Ret. = 0.04155854811853448\n",
      "(Annual) Avg. Ret. = 0.49870257742241375\n",
      "\n",
      "(Annual)  Sharpe = 2.825101390025434\n",
      "\n",
      "\n",
      "Panel D : Feb. 2024 - Feb. 2025\n",
      "H4 strategy\n",
      "(Monthly)  Avg. Ret. = 0.033978480136752136\n",
      "(Annual) Avg. Ret. = 0.4077417616410256\n",
      "\n",
      "L4 strategy\n",
      "(Monthly)  Avg. Ret. = 0.0069706876784188035\n",
      "(Annual) Avg. Ret. = 0.08364825214102564\n",
      "\n",
      "H4-L4 strategy\n",
      "(Monthly)  Avg. Ret. = 0.027007792458333332\n",
      "(Annual) Avg. Ret. = 0.3240935095\n",
      "\n",
      "(Annual)  Sharpe = 1.3716841989466635\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nPanel B : Mar. 2022 - Feb. 2023\")\n",
    "\n",
    "# Calculate H4 return \n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx:val_split_idx+232, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy().iloc[:232, :]\n",
    "signals.replace(-1, 0, inplace=True)\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "# print(signals.sum())\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"H4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_h4.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate L4 return\n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx:val_split_idx+232, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy().iloc[:232, :]\n",
    "signals.replace(1, 0, inplace=True)\n",
    "signals.replace(-1, 1, inplace=True)\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "# print(signals.sum())\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"L4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_l4.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate H4-L4 return\n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx:val_split_idx+232, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy().iloc[:232, :]\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "# print(signals.sum())\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"H4-L4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "print(\"(Annual)  Sharpe =\", np.sqrt(12) * strategy_returns[\"Portfolio\"].mean()/strategy_returns[\"Portfolio\"].std())\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_h4_l4.csv\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nPanel C : Mar. 2023 - Jan. 2024\")\n",
    "\n",
    "# Calculate H4 return \n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx+232:val_split_idx+464, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy().iloc[232:464, :]\n",
    "signals.replace(-1, 0, inplace=True)\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "# print(signals.sum())\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"H4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_h4.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate L4 return\n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx+232:val_split_idx+464, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy().iloc[232:464, :]\n",
    "signals.replace(1, 0, inplace=True)\n",
    "signals.replace(-1, 1, inplace=True)\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "# print(signals.sum())\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"L4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_l4.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate H4-L4 return\n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx+232:val_split_idx+464, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy().iloc[232:464, :]\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "# print(signals.sum())\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"H4-L4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "print(\"(Annual)  Sharpe =\", np.sqrt(12) * strategy_returns[\"Portfolio\"].mean()/strategy_returns[\"Portfolio\"].std())\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_h4_l4.csv\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nPanel D : Feb. 2024 - Feb. 2025\")\n",
    "\n",
    "# Calculate H4 return \n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx+464:, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy().iloc[464:, :]\n",
    "signals.replace(-1, 0, inplace=True)\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "# print(signals.sum())\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"H4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_h4.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate L4 return\n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx+464:, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy().iloc[464:, :]\n",
    "signals.replace(1, 0, inplace=True)\n",
    "signals.replace(-1, 1, inplace=True)\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "# print(signals.sum())\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"L4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_l4.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate H4-L4 return\n",
    "df_ret_org_test = df_ret_original.iloc[220:,:].iloc[val_split_idx+464:, :]\n",
    "\n",
    "signals = pd.DataFrame(y_pos_pred).copy().iloc[464:, :]\n",
    "\n",
    "signals.columns = df_ret_org_test.columns\n",
    "# print(signals.sum())\n",
    "\n",
    "signals.index = df_ret_org_test.index\n",
    "\n",
    "strategy_returns = (signals.shift(20) * df_ret_org_test)\n",
    "\n",
    "strategy_returns[\"Portfolio_sum\"] = strategy_returns.sum(axis=1)  # Aggregate portfolio returns\n",
    "strategy_returns[\"Portfolio\"] = strategy_returns[\"Portfolio_sum\"] / 4  # Aggregate portfolio returns\n",
    "strategy_returns[\"Cumulative Returns\"] = (1 + strategy_returns[\"Portfolio\"]).cumprod()\n",
    "\n",
    "print(\"H4-L4 strategy\")\n",
    "print(\"(Monthly)  Avg. Ret. =\", strategy_returns[\"Portfolio\"].mean())\n",
    "print(\"(Annual) Avg. Ret. =\", 12 * strategy_returns[\"Portfolio\"].mean())\n",
    "print()\n",
    "\n",
    "print(\"(Annual)  Sharpe =\", np.sqrt(12) * strategy_returns[\"Portfolio\"].mean()/strategy_returns[\"Portfolio\"].std())\n",
    "\n",
    "# Export\n",
    "df_result = pd.concat([df_ret_org_test, signals, strategy_returns], axis=1)\n",
    "\n",
    "base_col_names = df_ret_org_test.columns.tolist()\n",
    "df_result.columns = base_col_names + \\\n",
    "                    [col[:-4] + \"_signal_pred\" for col in base_col_names] + \\\n",
    "                    [col[:-4] + \"_strat_ret\" for col in base_col_names] + \\\n",
    "                    [\"Portfolio_sum\", \"Portfolio\", \"Cumulative Returns\"]\n",
    "\n",
    "df_result.to_csv(\"result_test_h4_l4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: torch.Size([698, 12, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SHAP 계산 중: 100%|██████████| 698/698 [00:44<00:00, 15.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP tensor 저장 완료: shap_tensor_momentum.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 모델 정의 (동일하게 유지) ===\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, num_classes, ffn_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.positional_encoding = PositionalEncoding(model_dim, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dim_feedforward=ffn_dim, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return self.softmax(x)\n",
    "\n",
    "# === 모델 불러오기 ===\n",
    "INPUT_DIM = 19\n",
    "MODEL_DIM = 56\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "FFN_DIM = 88\n",
    "NUM_CLASSES = 19\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = TransformerClassifier(INPUT_DIM, MODEL_DIM, NUM_HEADS, NUM_LAYERS, NUM_CLASSES, FFN_DIM, dropout=0.1)\n",
    "model.load_state_dict(torch.load(\"transformer_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === 데이터 불러오기 ===\n",
    "X_test = torch.load(\"X_test.pt\")  # shape = (T, 12, 19)\n",
    "X_test = X_test.to(device)\n",
    "T, SEQ_LEN, INPUT_DIM = X_test.shape\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# === SHAP Tensor 초기화 ===\n",
    "shap_tensor = np.zeros((T, NUM_CLASSES, INPUT_DIM))  # (시점, 예측자산, 인풋자산)\n",
    "\n",
    "# === SHAP 계산 ===\n",
    "for t in tqdm(range(T), desc=\"SHAP 계산 중\"):\n",
    "    x_input = X_test[t:t+1].numpy()  # (1, 12, 19)\n",
    "\n",
    "    for pred_asset in range(NUM_CLASSES):\n",
    "        # (1) 예측 함수 정의\n",
    "        \n",
    "        def predict_fn(x):\n",
    "            x_tensor = torch.tensor(x.reshape(-1, 12, 19), dtype=torch.float32)  # (1, 228) → (1, 12, 19)\n",
    "            return model(x_tensor).detach().numpy()[:, pred_asset]\n",
    "\n",
    "        # (2) 마스커 정의: (1, 12, 19) → (features = 12*19)\n",
    "        masker = shap.maskers.Independent(data=x_input.reshape(1, -1))  # 2D로 평탄화\n",
    "        explainer = shap.Explainer(predict_fn, masker, algorithm=\"permutation\")\n",
    "\n",
    "        # (3) 입력도 flatten해야 맞음\n",
    "        shap_vals = explainer(x_input.reshape(1, -1))\n",
    "\n",
    "        # (4) 다시 12x19로 reshape 후 평균 shap 계산\n",
    "        shap_matrix = shap_vals.values.reshape(12, 19)\n",
    "        shap_tensor[t, pred_asset, :] = shap_matrix.mean(axis=0)\n",
    "\n",
    "# === 저장 ===\n",
    "np.save(\"shap_tensor_momentum.npy\", shap_tensor)\n",
    "print(\"SHAP tensor 저장 완료: shap_tensor_momentum.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# === 1. SHAP tensor 로드 ===\n",
    "shap_tensor = np.load(\"shap_tensor_momentum.npy\")  # shape: (T=698, 19, 19)\n",
    "T, N_ASSETS, _ = shap_tensor.shape\n",
    "\n",
    "# === 2. 자산 이름 정의 (예시: 논문 기준) ===\n",
    "asset_names = ['CL', 'NG', 'HO', 'RB', 'XB', 'C', 'S', 'W', 'BO', 'SM', \n",
    "               'SB', 'KC', 'CT', 'LB', 'OJ', 'GC', 'SI', 'HG', 'PL']\n",
    "\n",
    "# === 3. 전체 평균 및 표준편차 계산 ===\n",
    "shap_mean = shap_tensor.mean(axis=0)  # shape: (19, 19)\n",
    "shap_std = shap_tensor.std(axis=0)    # shape: (19, 19)\n",
    "\n",
    "# === 4. Boxplot (전체 기간) ===\n",
    "os.makedirs(\"figs\", exist_ok=True)\n",
    "for i in range(N_ASSETS):  # 예측 대상 자산\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    sns.boxplot(data=shap_tensor[:, i, :])\n",
    "    plt.xticks(ticks=np.arange(N_ASSETS), labels=asset_names, rotation=45)\n",
    "    plt.title(f\"Boxplot of SHAP values (Predicted: {asset_names[i]})\")\n",
    "    plt.ylabel(\"SHAP value\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figs/boxplot_pred_{asset_names[i]}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# === 5. Heatmap (전체 평균 SHAP) ===\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(shap_mean, xticklabels=asset_names, yticklabels=asset_names, annot=True, fmt=\".3f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Heatmap of SHAP Mean Values (All Periods)\")\n",
    "plt.xlabel(\"Input Asset\")\n",
    "plt.ylabel(\"Predicted Asset\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/heatmap_shap_mean_all.png\")\n",
    "plt.close()\n",
    "\n",
    "# === 6. 구간 정의 (Panel B, C, D) ===\n",
    "# Panel B: t = 0~231, Panel C: 232~463, Panel D: 464~\n",
    "panel_ranges = {\n",
    "    \"full\": slice(None),\n",
    "    \"panel_b\": slice(0, 232),\n",
    "    \"panel_c\": slice(232, 464),\n",
    "    \"panel_d\": slice(464, None)\n",
    "}\n",
    "\n",
    "# === 7. Boxplot & Heatmap 저장 (각 구간별) ===\n",
    "for panel_name, panel_slice in panel_ranges.items():\n",
    "    panel_tensor = shap_tensor[panel_slice, :, :]\n",
    "    panel_mean = panel_tensor.mean(axis=0)\n",
    "\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(panel_mean, xticklabels=asset_names, yticklabels=asset_names, annot=True, fmt=\".3f\", cmap=\"coolwarm\")\n",
    "    plt.title(f\"Heatmap of SHAP Mean Values ({panel_name.upper()})\")\n",
    "    plt.xlabel(\"Input Asset\")\n",
    "    plt.ylabel(\"Predicted Asset\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figs/heatmap_shap_mean_{panel_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Boxplot (1장씩 19개 저장)\n",
    "    for i in range(N_ASSETS):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        sns.boxplot(data=panel_tensor[:, i, :])\n",
    "        plt.xticks(ticks=np.arange(N_ASSETS), labels=asset_names, rotation=45)\n",
    "        plt.title(f\"SHAP Boxplot ({panel_name.upper()}): Predicting {asset_names[i]}\")\n",
    "        plt.ylabel(\"SHAP value\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"figs/boxplot_{panel_name}_pred_{asset_names[i]}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[전체 기간] CL 예측에 가장 영향을 준 input feature 상위 5:\n",
      "1. JO: 평균 SHAP = 0.000000, 표준편차 = 0.000000\n",
      "2. KW: 평균 SHAP = 0.000000, 표준편차 = 0.000000\n",
      "3. NG: 평균 SHAP = 0.000000, 표준편차 = 0.000000\n",
      "4. HO: 평균 SHAP = 0.000000, 표준편차 = 0.000000\n",
      "5. RB: 평균 SHAP = 0.000000, 표준편차 = 0.000000\n",
      "\n",
      "전체 기간 평균 SHAP 행렬 (소수점 6자리 요약):\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "전체 기간 SHAP 표준편차 행렬 (소수점 6자리 요약):\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load SHAP tensor\n",
    "shap_tensor = np.load(\"shap_tensor_momentum.npy\")  # (698, 19, 19)\n",
    "T, N, _ = shap_tensor.shape\n",
    "\n",
    "# 전체 평균 및 표준편차 계산\n",
    "shap_mean = shap_tensor.mean(axis=0)  # (19, 19)\n",
    "shap_std = shap_tensor.std(axis=0)    # (19, 19)\n",
    "\n",
    "# 자산 이름 지정\n",
    "asset_names = [\"CL\", \"NG\", \"HO\", \"RB\", \"XB\", \"C\", \"S\", \"W\", \"KW\", \n",
    "               \"SB\", \"LB\", \"SM\", \"BO\", \"FC\", \"LH\", \"LC\", \"OJ\", \"CC\", \"JO\"]\n",
    "\n",
    "# 예측 자산 CL 기준으로 input feature SHAP value 평균 상위 5개 출력\n",
    "pred_asset_idx = asset_names.index(\"CL\")\n",
    "mean_row = shap_mean[pred_asset_idx]\n",
    "top5_idx = mean_row.argsort()[::-1][:5]\n",
    "\n",
    "print(f\"\\n[전체 기간] CL 예측에 가장 영향을 준 input feature 상위 5:\")\n",
    "for rank, idx in enumerate(top5_idx, 1):\n",
    "    print(f\"{rank}. {asset_names[idx]}: 평균 SHAP = {mean_row[idx]:.6f}, 표준편차 = {shap_std[pred_asset_idx][idx]:.6f}\")\n",
    "\n",
    "# 전체 SHAP 평균 행렬 요약 출력\n",
    "print(\"\\n전체 기간 평균 SHAP 행렬 (소수점 6자리 요약):\")\n",
    "print(np.round(shap_mean, 6))\n",
    "\n",
    "# 전체 SHAP 표준편차 행렬 요약 출력\n",
    "print(\"\\n전체 기간 SHAP 표준편차 행렬 (소수점 6자리 요약):\")\n",
    "print(np.round(shap_std, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모든 heatmap 및 boxplot 저장 완료 → ./shap_output 폴더 확인\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load\n",
    "shap_tensor = np.load(\"shap_tensor_momentum.npy\")  # (T, 19, 19)\n",
    "T = shap_tensor.shape[0]\n",
    "asset_names = [\"CL\", \"NG\", \"HO\", \"RB\", \"XB\", \"C\", \"S\", \"W\", \"KW\", \n",
    "               \"SB\", \"LB\", \"SM\", \"BO\", \"FC\", \"LH\", \"LC\", \"OJ\", \"CC\", \"JO\"]\n",
    "\n",
    "# Create folder\n",
    "os.makedirs(\"shap_output\", exist_ok=True)\n",
    "\n",
    "# === 전체 구간 평균/표준편차 ===\n",
    "shap_mean = shap_tensor.mean(axis=0)  # (19, 19)\n",
    "shap_std = shap_tensor.std(axis=0)\n",
    "\n",
    "# 🔷 전체 구간 Heatmap 저장\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(shap_mean, xticklabels=asset_names, yticklabels=asset_names, cmap=\"YlGnBu\", annot=False)\n",
    "plt.title(\"Heatmap: SHAP Mean (All Period)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_output/heatmap_all_period.png\")\n",
    "plt.close()\n",
    "\n",
    "# 🔷 전체 구간 Boxplot 저장\n",
    "shap_reshaped = shap_tensor.reshape(-1, 19)\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=shap_reshaped)\n",
    "plt.title(\"Boxplot: SHAP Value (All Period)\")\n",
    "plt.xticks(ticks=np.arange(19), labels=asset_names, rotation=45)\n",
    "plt.ylabel(\"SHAP value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_output/boxplot_all_period.png\")\n",
    "plt.close()\n",
    "\n",
    "# === 구간별 (Full / Sub1 / Sub2 / Sub3) ===\n",
    "periods = {\n",
    "    \"full\": (0, T),\n",
    "    \"sub1\": (0, 232),\n",
    "    \"sub2\": (232, 464),\n",
    "    \"sub3\": (464, 698)\n",
    "}\n",
    "\n",
    "for label, (start, end) in periods.items():\n",
    "    period_data = shap_tensor[start:end]\n",
    "    period_mean = period_data.mean(axis=0)\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(period_mean, xticklabels=asset_names, yticklabels=asset_names, cmap=\"YlOrBr\", annot=False)\n",
    "    plt.title(f\"Heatmap: SHAP Mean ({label})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"shap_output/heatmap_{label}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Boxplot\n",
    "    period_flat = period_data.reshape(-1, 19)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.boxplot(data=period_flat)\n",
    "    plt.title(f\"Boxplot: SHAP Value ({label})\")\n",
    "    plt.xticks(ticks=np.arange(19), labels=asset_names, rotation=45)\n",
    "    plt.ylabel(\"SHAP value\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"shap_output/boxplot_{label}.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"✅ 모든 heatmap 및 boxplot 저장 완료 → ./shap_output 폴더 확인\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30920,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
